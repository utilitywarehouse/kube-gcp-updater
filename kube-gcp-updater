#!/bin/bash

# Shell style guide: https://google.github.io/styleguide/shell.xml

# ## (-) master and worker steps
# ## (*) worker specific steps
#
# Assumes manual gcp kubernetes cluster with master and workers nodes driven
# by regionInstanceGroupManagers
#
# - label nodes
# - disable scheduling
# - double regionInstanceGroupManagers size
# - wait for 2x group
# - drain labelled nodes (sequentially)
# - terminate all old nodes by reducing respective regionIGM size to half again

set -o errexit
set -o nounset
set -o pipefail

readonly retire_time=$(date +"%Y-%m-%dT%H-%M-%SZ")

# flags
kube_context=''
gcp_project=''
role=''
resume=''
timeout=600

usage() {
  cat <<EOF
Usage: $0 -c <kube_context> -p <gcp_project> -s [retire_time_label] -r [role]
        -t [timeout]

  -s    Resume node rolling. Argument is retire_time label. If set, -r [role]
        must also be set
  -t    Node drain timeout. How long to wait for the node to drain before
        shutting it down (in seconds, default 300s)
EOF
}

while getopts 'c:p:r:hs:t:' flag; do
  case "${flag}" in
    c) kube_context="${OPTARG}" ;;
    p) gcp_project="${OPTARG}" ;;
    r) role="${OPTARG}" ;;
    s) resume="${OPTARG}" ;;
    t) timeout="${OPTARG}" ;;
    h) usage && exit 0 ;;
    *) echo "Unexpected option: ${flag}" && usage && exit 1 ;;
  esac
done

### Deps
deps="
  awk
  jq
  kubectl
  gcloud
  timeout
  xargs
"

missing_deps=""
for d in ${deps}; do
  if ! command -v ${d} &> /dev/null; then
    missing_deps+="${d} "
  fi
done

if [[ -n "${missing_deps}" ]]; then
  echo "Missing dependencies: ${missing_deps}"
  exit 1
fi

###

### Validation
if [[ -z "${kube_context}" ]]; then
  usage
  exit 1
fi

if [[ -n "${resume}" ]] && [[ -z "${role}" ]] ; then
  echo "If you are resuming, you need to provide a role"
  usage
  exit 1
fi

if [[ -z "${gcp_project}" ]]; then
  usage
  exit 1
fi

# Verify we can successfully authenticate with Google
gcloud projects describe "${gcp_project}"

function fail() {
  echo $1 >&2
  exit 1
}

function retry() {
  local n=1
  local max=12
  local delay=8
  while true; do
    "$@" && break || {
      if [[ $n -lt $max ]]; then
        ((n++))
        echo "command failed: attempt=$n max=$max"
        sleep $delay;
      else
        fail "the command has failed after $n attempts."
      fi
    }
  done
}

label_for_cycling() {
  local role=$1
  local nodes=""
  while [[ -z "${nodes}" ]]; do
    nodes=$(retry kubectl --context=${kube_context} get nodes -l role=${role} -o json | jq -r '.items[].metadata.name')
  done

  echo "${kube_context}: nodes=$(echo "${nodes}" | wc -l) role=${role}"
  echo "labelling for retirement: role=${role}"
  for node in ${nodes}; do
    retry kubectl --context=${kube_context} label node ${node} retiring=${retire_time} --overwrite=true
    retry kubectl --context=${kube_context} cordon ${node}
  done
}

delete_pods() {
  local node=$1

  retry kubectl --context=${kube_context} get pod --all-namespaces \
    -o jsonpath='{range .items[?(.spec.nodeName=="'${node}'")]}{@.metadata.namespace} {.metadata.name} {end}' |\
    xargs -n2 |\
    xargs -I % sh -c "kubectl --context=${kube_context} delete pods -n=%"
}

drain_node() {
  local node=$1

  set +e
  time timeout ${timeout} kubectl --context=${kube_context} drain ${node} --ignore-daemonsets --force --delete-local-data
  local rc=$?
  if [[ ${rc} -eq 0 ]]; then
    echo "drained successfully"
  elif [[ ${rc} -eq 124 ]]; then
    echo "timeout reached, continuing: timeout=${timeout}"
    delete_pods ${node}
  else
    echo "kubectl drain exit error: ${rc}"
    delete_pods ${node}
  fi
}

# Kill old nodes at once to avoid automatic gcp rebalancing (can't be disabled).
# Rebalancing means that gcp will try to create new nodes and delete existing ones
# in order to spread them evenly across zones.
# Killing as a batch operation is supported for up to 100 instances at once.
kill_old_nodes() {

  set +e
  local nodes_to_kill=""
  while [[ -z "${nodes_to_kill}" ]]; do
    nodes_to_kill=$(retry kubectl --context=${kube_context} get nodes -l role="${role}",retiring="${retire_time}" -o json |\
      jq -r '.items[].metadata.name' |\
      awk -F'[.]' '{print $1}' |\
      awk 'FNR!=1{print l}{l=$0};END{ORS="";print l}' ORS=',')
  done

  local instance_address=""
  while [[ -z "${instance_address}" ]]; do
    instance_address=$(retry kubectl --context=${kube_context} get nodes -l role=${role},retiring=${retire_time} -o json |\
      jq -r '.items[0].metadata.name')
  done

  local instance_zone=""
  while [[ -z "${instance_zone}" ]]; do
    instance_zone=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["failure-domain.beta.kubernetes.io/zone"]')
  done

  local instance_region=""
  while [[ -z "${instance_region}" ]]; do
    instance_region=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["failure-domain.beta.kubernetes.io/region"]')
  done

  local instance_id=""
  while [[ -z "${instance_id}" ]]; do
    instance_id=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["kubernetes.io/hostname"]' |\
      awk -F'[.]' '{print $1}')
  done
  set -e

  local instance_group_manager=$(retry gcloud compute instances describe ${instance_id} --zone=${instance_zone} --project=${gcp_project} --format=json |\
    jq -r '.metadata.items[] | select(.key=="created-by") | .value' | awk -F'[/]' '{print $NF}')

  echo "gcp terminating nodes: ${nodes_to_kill}"
  retry gcloud compute instance-groups managed delete-instances ${instance_group_manager} --instances=${nodes_to_kill} --region=${instance_region} --project=${gcp_project}
  echo "-------------------------------------------------------------" 

  echo "Wait until ${instance_group_manager} is stable"
  retry gcloud compute instance-groups managed wait-until-stable ${instance_group_manager} --region=${instance_region} --project=${gcp_project}

}

cycle_nodes() {
  local role=$1

  # - double regionIGM size
  set +e
  local instance_address=""
  while [[ -z "${instance_address}" ]]; do
    instance_address=$(retry kubectl --context=${kube_context} get nodes -l role=${role},retiring=${retire_time} -o json |\
      jq -r '.items[0].metadata.name')
  done

  local instance_zone=""
  while [[ -z "${instance_zone}" ]]; do
    instance_zone=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["failure-domain.beta.kubernetes.io/zone"]')
  done

  local instance_region=""
  while [[ -z "${instance_region}" ]]; do
    instance_region=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["failure-domain.beta.kubernetes.io/region"]')
  done

  local instance_id=""
  while [[ -z "${instance_id}" ]]; do
    instance_id=$(retry kubectl --context=${kube_context} get node ${instance_address} -o json |\
      jq -r '.metadata.labels["kubernetes.io/hostname"]' | awk -F'[.]' '{print $1}')
  done
  set -e

  local instance_group_manager=$(retry gcloud compute instances describe ${instance_id} --zone=${instance_zone} --project=${gcp_project} --format=json |\
    jq -r '.metadata.items[] | select(.key=="created-by") | .value' | awk -F'[/]' '{print $NF}')

  ## Assumes regional Instance Group Manager
  local igm_targetSize=$(retry gcloud compute instance-groups managed describe ${instance_group_manager} --region=${instance_region} --project=${gcp_project} --format=json |\
    jq -r .targetSize)

  echo "-------------------------------------------------------------"
  echo "Doubling the size of group manager ${instance_group_manager}:"
  retry gcloud compute instance-groups managed resize ${instance_group_manager} --size=$(( ${igm_targetSize} * 2)) --region=${instance_region} --project=${gcp_project}

  # - wait for 2x group
  set +e
  local count=0
  while [[ ${count} -lt ${igm_targetSize} ]]; do
    sleep 32
    count=$(retry kubectl --context=${kube_context} get node -lrole=${role} -Lrole,retiring |\
      grep -v ${retire_time} |\
      grep -v NotReady |\
      grep Ready |\
      wc -l)
    echo "waiting for new nodes: actual=${count} expected=${igm_targetSize} IMG=\"${instance_group_manager}\""
  done
  set -e

  # - drain labelled nodes (sequentially)
  local old_nodes=$(retry kubectl --context="${kube_context}" get nodes -l role="${role}",retiring="${retire_time}" -o json |\
    jq -r '.items[].metadata.name')
  for old_node in ${old_nodes}; do
    drain_node ${old_node}
  done

  # Kill old nodes
  kill_old_nodes

  # wait for kubelet node count to catch up
  set +e
  local count=0
  while [[ ${count} -ne ${igm_targetSize} ]]; do
    sleep 32
    count=$(retry kubectl --context=${kube_context} get node -lrole=${role} -Lrole |\
      grep  ${role} |\
      wc -l)
    echo "waiting kube controller to update nodes: actual=${count} expected=${igm_targetSize}"
  done
  set -e
}

update() {
  local role=$1
  label_for_cycling ${role}
  cycle_nodes ${role}
}

resume() {
  local old_nodes=$(retry kubectl --context="${kube_context}" get nodes -l role="${role}",retiring="${resume}" -o json |\
    jq -r '.items[].metadata.name')
  for old_node in ${old_nodes}; do
    drain_node ${old_node}
  done

}

echo "kube cluster: ${kube_context}"

if [[ -n "${resume}" ]]; then
  resume
  exit 0
fi

if [[ -n "${role}" ]]; then
  update ${role}
else
  update master
  update worker
fi

echo "run: result=\"success\""
